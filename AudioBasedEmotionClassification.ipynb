{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AudioBasedEmotionClassification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPqICo2h56AC",
        "colab_type": "code",
        "outputId": "8fa8dd9f-5020-4258-e846-e3840a47f46d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHSbKMLc8SRH",
        "colab_type": "code",
        "outputId": "fa08acd8-13fd-465c-fb81-f91f90df65f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# import os\n",
        "# import time\n",
        "# import numpy as np\n",
        "# import librosa\n",
        "# lst = []\n",
        "\n",
        "# start_time = time.time()\n",
        "# for folder in os.listdir('.'):\n",
        "#   if folder[0]=='A':\n",
        "#     for file in os.listdir(folder):\n",
        "#         try:\n",
        "#           X, sample_rate = librosa.load(os.path.join(folder,file), res_type='kaiser_fast')\n",
        "#           mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
        "#           file = int(file[7:8]) - 1 \n",
        "#           arr = mfccs, file\n",
        "#           lst.append(arr)\n",
        "#         except ValueError:\n",
        "#           continue\n",
        "\n",
        "# print(\"--- Data loaded. Loading time: %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Data loaded. Loading time: 236.32492113113403 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rXgCDDoFaBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X, y = zip(*lst)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEd5X98bboIW",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7QlrAFBL5yj",
        "colab_type": "code",
        "outputId": "cb8ef9f6-1db0-4773-82c0-e07fbd615469",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# import numpy as np\n",
        "# X = np.asarray(X)\n",
        "# y = np.asarray(y)\n",
        "\n",
        "\n",
        "# X.shape, y.shape\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1440, 40), (1440,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxjLjrQOj-45",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install oblib\n",
        "# import joblib\n",
        "\n",
        "# X_name = 'X.joblib'\n",
        "# y_name = 'y.joblib'\n",
        "# save_dir = '/content/drive/My Drive/ravdess_model'\n",
        "\n",
        "# savedX = joblib.dump(X, os.path.join(save_dir, X_name))\n",
        "# savedy = joblib.dump(y, os.path.join(save_dir, y_name))\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import joblib\n",
        "# np.save('gdrive/My Drive/ravdess_model/x',X)\n",
        "# np.save('gdrive/My Drive/ravdess_model/y',y)\n",
        "\n",
        "X = joblib.load('gdrive/My Drive/ravdess_model/X.joblib')\n",
        "y = joblib.load('gdrive/My Drive/ravdess_model/y.joblib')\n",
        "\n",
        "# for i in range(len(y)):\n",
        "#   if y[i]>0:\n",
        "#     y[i]-=1\n",
        "\n",
        "\n",
        "\n",
        "# array_rain_fall = np.loadtxt(fname=\"rain-fall.csv\", delimiter=\",\")\n",
        "# np.savetxt(fname=\"saved-rain-fall-row-col-names.csv\", delimiter=\",\", X=array_rain_fall)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tR0GIprwnhij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "columns = range(len(y))\n",
        "df = pd.DataFrame(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_czf1LjnqLM",
        "colab_type": "code",
        "outputId": "5010ca0b-8563-4482-aca2-18339bf6411f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        }
      },
      "source": [
        "df.tail()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4943</th>\n",
              "      <td>-646.931546</td>\n",
              "      <td>61.377081</td>\n",
              "      <td>-16.988777</td>\n",
              "      <td>12.555034</td>\n",
              "      <td>-13.093199</td>\n",
              "      <td>3.082647</td>\n",
              "      <td>-11.279306</td>\n",
              "      <td>-10.540529</td>\n",
              "      <td>-9.113195</td>\n",
              "      <td>2.369075</td>\n",
              "      <td>-8.408164</td>\n",
              "      <td>-1.000017</td>\n",
              "      <td>-9.220102</td>\n",
              "      <td>0.270728</td>\n",
              "      <td>-4.869185</td>\n",
              "      <td>-3.798038</td>\n",
              "      <td>-4.465536</td>\n",
              "      <td>-2.551504</td>\n",
              "      <td>-6.219362</td>\n",
              "      <td>-2.610812</td>\n",
              "      <td>-3.215108</td>\n",
              "      <td>-4.511744</td>\n",
              "      <td>-6.140197</td>\n",
              "      <td>-2.663885</td>\n",
              "      <td>-3.467748</td>\n",
              "      <td>-2.806494</td>\n",
              "      <td>-0.882522</td>\n",
              "      <td>-1.555897</td>\n",
              "      <td>2.077955</td>\n",
              "      <td>1.014428</td>\n",
              "      <td>-0.226662</td>\n",
              "      <td>1.816127</td>\n",
              "      <td>0.245245</td>\n",
              "      <td>2.198746</td>\n",
              "      <td>2.465535</td>\n",
              "      <td>2.617508</td>\n",
              "      <td>1.774237</td>\n",
              "      <td>0.930601</td>\n",
              "      <td>0.747460</td>\n",
              "      <td>1.498267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4944</th>\n",
              "      <td>-675.536781</td>\n",
              "      <td>48.541577</td>\n",
              "      <td>-3.693695</td>\n",
              "      <td>15.501362</td>\n",
              "      <td>-7.372083</td>\n",
              "      <td>-1.366871</td>\n",
              "      <td>-14.761905</td>\n",
              "      <td>-5.579250</td>\n",
              "      <td>-5.142587</td>\n",
              "      <td>-1.491627</td>\n",
              "      <td>-10.421262</td>\n",
              "      <td>-1.190409</td>\n",
              "      <td>-2.440685</td>\n",
              "      <td>-4.300209</td>\n",
              "      <td>-3.974783</td>\n",
              "      <td>-3.007353</td>\n",
              "      <td>-5.499747</td>\n",
              "      <td>-0.201112</td>\n",
              "      <td>-6.016098</td>\n",
              "      <td>-2.445127</td>\n",
              "      <td>-3.699853</td>\n",
              "      <td>-3.571032</td>\n",
              "      <td>-3.403845</td>\n",
              "      <td>-5.400430</td>\n",
              "      <td>-3.425282</td>\n",
              "      <td>-1.115717</td>\n",
              "      <td>1.684772</td>\n",
              "      <td>2.438847</td>\n",
              "      <td>2.609585</td>\n",
              "      <td>1.804672</td>\n",
              "      <td>-0.563119</td>\n",
              "      <td>1.706292</td>\n",
              "      <td>0.030133</td>\n",
              "      <td>1.093912</td>\n",
              "      <td>0.216716</td>\n",
              "      <td>1.608424</td>\n",
              "      <td>-1.338693</td>\n",
              "      <td>0.192503</td>\n",
              "      <td>-0.731679</td>\n",
              "      <td>1.501951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4945</th>\n",
              "      <td>-608.778315</td>\n",
              "      <td>59.247194</td>\n",
              "      <td>-6.208647</td>\n",
              "      <td>10.505940</td>\n",
              "      <td>-10.760311</td>\n",
              "      <td>2.287540</td>\n",
              "      <td>-14.015711</td>\n",
              "      <td>-9.075580</td>\n",
              "      <td>-8.449490</td>\n",
              "      <td>4.014549</td>\n",
              "      <td>-10.895445</td>\n",
              "      <td>-3.131255</td>\n",
              "      <td>-2.431987</td>\n",
              "      <td>-2.044737</td>\n",
              "      <td>-5.594034</td>\n",
              "      <td>-2.888829</td>\n",
              "      <td>-7.209847</td>\n",
              "      <td>-3.264448</td>\n",
              "      <td>-5.759219</td>\n",
              "      <td>-1.175307</td>\n",
              "      <td>-3.472448</td>\n",
              "      <td>-3.047401</td>\n",
              "      <td>-3.885690</td>\n",
              "      <td>-4.228245</td>\n",
              "      <td>-4.744638</td>\n",
              "      <td>-1.849893</td>\n",
              "      <td>-2.489592</td>\n",
              "      <td>-1.204477</td>\n",
              "      <td>-2.144263</td>\n",
              "      <td>-2.033505</td>\n",
              "      <td>-1.972012</td>\n",
              "      <td>-0.051326</td>\n",
              "      <td>-1.002053</td>\n",
              "      <td>-0.190803</td>\n",
              "      <td>0.205867</td>\n",
              "      <td>2.353034</td>\n",
              "      <td>3.459262</td>\n",
              "      <td>3.242251</td>\n",
              "      <td>2.071721</td>\n",
              "      <td>2.525186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4946</th>\n",
              "      <td>-560.837177</td>\n",
              "      <td>50.168745</td>\n",
              "      <td>-19.705780</td>\n",
              "      <td>4.857206</td>\n",
              "      <td>-14.152938</td>\n",
              "      <td>-1.806413</td>\n",
              "      <td>-14.607022</td>\n",
              "      <td>-10.531889</td>\n",
              "      <td>-8.830826</td>\n",
              "      <td>1.842285</td>\n",
              "      <td>-10.997461</td>\n",
              "      <td>3.024669</td>\n",
              "      <td>-11.034260</td>\n",
              "      <td>0.173616</td>\n",
              "      <td>-5.544484</td>\n",
              "      <td>-4.531592</td>\n",
              "      <td>-1.218367</td>\n",
              "      <td>-4.226231</td>\n",
              "      <td>-7.439676</td>\n",
              "      <td>-2.918094</td>\n",
              "      <td>-4.972704</td>\n",
              "      <td>-4.242082</td>\n",
              "      <td>-4.365048</td>\n",
              "      <td>-2.168633</td>\n",
              "      <td>-3.950433</td>\n",
              "      <td>-0.162558</td>\n",
              "      <td>2.696078</td>\n",
              "      <td>3.357330</td>\n",
              "      <td>4.228732</td>\n",
              "      <td>5.612513</td>\n",
              "      <td>6.008360</td>\n",
              "      <td>8.070698</td>\n",
              "      <td>5.162092</td>\n",
              "      <td>4.522226</td>\n",
              "      <td>2.342330</td>\n",
              "      <td>0.923058</td>\n",
              "      <td>1.369484</td>\n",
              "      <td>1.216854</td>\n",
              "      <td>0.766931</td>\n",
              "      <td>0.479213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4947</th>\n",
              "      <td>-606.634919</td>\n",
              "      <td>61.005785</td>\n",
              "      <td>-24.945132</td>\n",
              "      <td>13.234665</td>\n",
              "      <td>-5.295787</td>\n",
              "      <td>-4.091784</td>\n",
              "      <td>-7.111942</td>\n",
              "      <td>-8.966161</td>\n",
              "      <td>-10.679787</td>\n",
              "      <td>3.256634</td>\n",
              "      <td>-9.203079</td>\n",
              "      <td>2.093423</td>\n",
              "      <td>-12.159755</td>\n",
              "      <td>-1.357020</td>\n",
              "      <td>-2.136598</td>\n",
              "      <td>-7.099506</td>\n",
              "      <td>-7.953842</td>\n",
              "      <td>-1.035362</td>\n",
              "      <td>-5.483356</td>\n",
              "      <td>-2.363346</td>\n",
              "      <td>-0.067038</td>\n",
              "      <td>1.916906</td>\n",
              "      <td>-0.763467</td>\n",
              "      <td>2.335582</td>\n",
              "      <td>-1.165362</td>\n",
              "      <td>1.852167</td>\n",
              "      <td>0.116237</td>\n",
              "      <td>2.224703</td>\n",
              "      <td>2.673086</td>\n",
              "      <td>6.556891</td>\n",
              "      <td>5.686625</td>\n",
              "      <td>7.073574</td>\n",
              "      <td>7.005492</td>\n",
              "      <td>8.724262</td>\n",
              "      <td>6.402209</td>\n",
              "      <td>3.748414</td>\n",
              "      <td>0.912942</td>\n",
              "      <td>0.379323</td>\n",
              "      <td>0.921929</td>\n",
              "      <td>1.557195</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              0          1          2   ...        37        38        39\n",
              "4943 -646.931546  61.377081 -16.988777  ...  0.930601  0.747460  1.498267\n",
              "4944 -675.536781  48.541577  -3.693695  ...  0.192503 -0.731679  1.501951\n",
              "4945 -608.778315  59.247194  -6.208647  ...  3.242251  2.071721  2.525186\n",
              "4946 -560.837177  50.168745 -19.705780  ...  1.216854  0.766931  0.479213\n",
              "4947 -606.634919  61.005785 -24.945132  ...  0.379323  0.921929  1.557195\n",
              "\n",
              "[5 rows x 40 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q1UX263Gfsy",
        "colab_type": "code",
        "outputId": "96426f19-26a9-44b7-ad83-75f8c9bfc41e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from fastai.basics import *\n",
        "from fastai.vision import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "\n",
        "class ArrayDataset(Dataset):\n",
        "    \"Sample numpy array dataset\"\n",
        "    def __init__(self, x, y):\n",
        "        self.x, self.y = x, y\n",
        "        self.c = 2 # binary label\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        return self.x[i], self.y[i]\n",
        "      \n",
        "\n",
        "\n",
        "    \n",
        "train_X,valid_X, train_y, valid_y = train_test_split(df,y,test_size=0.2,random_state=42)\n",
        "x_traincnn = np.expand_dims(train_X, axis=2)\n",
        "x_validcnn = np.expand_dims(valid_X, axis=2)\n",
        "# X = np.expand_dims(X,axis=2)\n",
        "print(train_y,valid_y)\n",
        "# Load necessary Pytorch packages\n",
        "# from torch.utils.data import DataLoader, TensorDataset\n",
        "# from torch import Tensor\n",
        "\n",
        "# # Create dataset from several tensors with matching first dimension\n",
        "# # Samples will be drawn from the first dimension (rows)\n",
        "# train_dataset = TensorDataset( Tensor(x_traincnn), Tensor(train_y) )\n",
        "# valid_dataset = TensorDataset( Tensor(x_validcnn), Tensor(valid_y) )\n",
        "# # Create a data loader from the dataset\n",
        "# # Type of sampling and batch size are specified at this step\n",
        "# train_loader = DataLoader(train_dataset, batch_size= 16, shuffle=True)\n",
        "# valid_loader = DataLoader(valid_dataset, batch_size= 16, shuffle=True)\n",
        "\n",
        "# # Quick test\n",
        "# next(iter(train_loader))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# x_traincnn = torch.Tensor([x_traincnn,train_y])\n",
        "# x_testcnn = torch.Tensor([x_testcnn,valid_y])\n",
        "# # print(type(x_traincnn))\n",
        "# print(x_traincnn)\n",
        "# train_set,valid_set = ArrayDataset(train_X,train_y),ArrayDataset(valid_X,valid_y)\n",
        "\n",
        "# train_data = []\n",
        "# valid_data = []\n",
        "# for i in range(len(train_X)):\n",
        "#    train_data.append([train_X[i], train_y[i]])\n",
        "# for i in range(len(valid_X)):\n",
        "#    valid_data.append([valid_X[i], valid_y[i]])\n",
        "    \n",
        "    \n",
        "# train_data = np.array(train_data)\n",
        "# valid_data = np.array(valid_data)\n",
        "\n",
        "\n",
        "# train_data = np.expand_dims(train_data, axis=2)\n",
        "# valid_data = np.expand_dims(valid_data, axis=2)\n",
        "\n",
        "# print(train_data.shape)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(x_traincnn, batch_size=16,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "validloader = torch.utils.data.DataLoader(x_validcnn, batch_size=16,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4 3 3 5 ... 3 2 2 5] [1 4 3 0 ... 2 3 1 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr0qEEhnprBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv(ni,nf): return nn.Conv1d(ni, nf, kernel_size=3, stride=2, padding=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6U1wn2KplTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = nn.Sequential(\n",
        "    conv(1, 8), # 14\n",
        "    nn.BatchNorm2d(8),\n",
        "    nn.ReLU(),\n",
        "    conv(8, 16), # 7\n",
        "    nn.BatchNorm2d(16),\n",
        "    nn.ReLU(),\n",
        "    conv(16, 32), # 4\n",
        "    nn.BatchNorm2d(32),\n",
        "    nn.ReLU(),\n",
        "    conv(32, 16), # 2\n",
        "    nn.BatchNorm2d(16),\n",
        "    nn.ReLU(),\n",
        "    conv(16, 8), # 1\n",
        "    nn.BatchNorm2d(8),\n",
        "    Flatten(),   # remove (1,1) grid\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6cyetGQLKRV",
        "colab_type": "text"
      },
      "source": [
        "Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1spZSHeqC0D",
        "colab_type": "code",
        "outputId": "81a78ff1-e53f-4d86-9613-b4698a25b0fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv1d(1, 8, kernel_size=(3,), stride=(2,), padding=(1,))\n",
              "  (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (2): ReLU()\n",
              "  (3): Conv1d(8, 16, kernel_size=(3,), stride=(2,), padding=(1,))\n",
              "  (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (5): ReLU()\n",
              "  (6): Conv1d(16, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
              "  (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (8): ReLU()\n",
              "  (9): Conv1d(32, 16, kernel_size=(3,), stride=(2,), padding=(1,))\n",
              "  (10): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (11): ReLU()\n",
              "  (12): Conv1d(16, 8, kernel_size=(3,), stride=(2,), padding=(1,))\n",
              "  (13): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (14): Flatten()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7olgNNvrrx7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3nUyDsLsvDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 10\n",
        "#3958\n",
        "i=0\n",
        "while i<3950:\n",
        "  inputs,labels = x_traincnn[i:(i+batch_size)],train_y[i:(i+batch_size)]\n",
        "  inputs = torch.from_numpy(inputs)\n",
        "  labels = torch.from_numpy(labels)\n",
        "  print(inputs.shape,labels.shape)\n",
        "  i+=10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lPMk3FrqyNM",
        "colab_type": "code",
        "outputId": "b7360092-9e3e-4382-9b44-d2c0c13c4bfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "source": [
        "for epoch in range(10):  # loop over the dataset multiple times\n",
        "  running_loss = 0.0\n",
        "  batch_size = 10\n",
        "  #3958\n",
        "  i=0\n",
        "  while i<3950:\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "    inputs,labels = x_traincnn[i:(i+batch_size)],train_y[i:(i+batch_size)]\n",
        "    inputs = torch.from_numpy(inputs)\n",
        "    labels = torch.from_numpy(labels)\n",
        "    # forward + backward + optimize\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    i+=10\n",
        "# print statistics\n",
        "    running_loss += loss.item()\n",
        "    if i % 100 == 99:    # print every 2000 mini-batches\n",
        "      print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
        "      running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 40, 1]) torch.Size([10])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-4b72f02c48e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    194\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    195\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 196\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size 8 1 3, expected input[10, 40, 1] to have 1 channels, but got 40 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoXEuFie4sjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class resblock(Layer):\n",
        "    def __init__(self,nf, **kwargs):\n",
        "        super(resblock, self).__init__()\n",
        "\n",
        "\n",
        "    def call(self, nf, nf):\n",
        "      \n",
        "      \n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'axis': self.axis,\n",
        "            'momentum': self.momentum,\n",
        "            'epsilon': self.epsilon,\n",
        "            'center': self.center,\n",
        "            'scale': self.scale,\n",
        "            'beta_initializer': initializers.serialize(self.beta_initializer),\n",
        "            'gamma_initializer': initializers.serialize(self.gamma_initializer),\n",
        "            'moving_mean_initializer':\n",
        "                initializers.serialize(self.moving_mean_initializer),\n",
        "            'moving_variance_initializer':\n",
        "                initializers.serialize(self.moving_variance_initializer),\n",
        "            'beta_regularizer': regularizers.serialize(self.beta_regularizer),\n",
        "            'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n",
        "            'beta_constraint': constraints.serialize(self.beta_constraint),\n",
        "            'gamma_constraint': constraints.serialize(self.gamma_constraint)\n",
        "        }\n",
        "        base_config = super(BatchNormalization, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7ahaIohFNWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv1D(32, 5,strides=2,padding='same',input_shape=(40,1)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "# model.add(x)\n",
        "# model.add(MaxPooling1D(pool_size=(8)))\n",
        "model.add(Conv1D(64, 5,strides=2,padding='same')\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv1D(8, 5,strides=2,padding='same',))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Flatten())\n",
        "model.add(Dense(8))\n",
        "model.add(Activation('softmax'))\n",
        "# model.add(Dense(64))\n",
        "# model.add(Activation('relu'))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(Dense(7))\n",
        "\n",
        "opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2WaO-264hlg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BatchNormalization??"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AP6ddSvpFgav",
        "colab_type": "code",
        "outputId": "be1d7fae-d089-41ca-ac65-1849c354b754",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_1 (Conv1D)            (None, 20, 32)            192       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 20, 32)            0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 20, 32)            128       \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 10, 64)            10304     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 10, 64)            0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 10, 64)            256       \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 5, 8)              2568      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 5, 8)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 5, 8)              32        \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 40)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 8)                 328       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 8)                 0         \n",
            "=================================================================\n",
            "Total params: 13,808\n",
            "Trainable params: 13,600\n",
            "Non-trainable params: 208\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6McgEjjMG1_J",
        "colab_type": "code",
        "outputId": "d6f6ba2a-b203-4b3c-8092-5fd3a35a310c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXomkbELJI4q",
        "colab_type": "code",
        "outputId": "dbcbdd1c-6d58-459e-fc15-e7816ad94b8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras import backend as K\n",
        "K.tensorflow_backend._get_available_gpus()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/job:localhost/replica:0/task:0/device:GPU:0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaDI3U7RHJT2",
        "colab_type": "code",
        "outputId": "e02c7fdf-ab8a-48d4-bac4-8f09d81fb9f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "cnnhistory=model.fit(x_traincnn, train_y, batch_size=16, epochs=100, validation_data=(x_validcnn, valid_y))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 3958 samples, validate on 990 samples\n",
            "Epoch 1/100\n",
            "3958/3958 [==============================] - 1s 246us/step - loss: 0.0167 - acc: 0.9932 - val_loss: 0.2625 - val_acc: 0.9495\n",
            "Epoch 2/100\n",
            "3958/3958 [==============================] - 1s 257us/step - loss: 0.0237 - acc: 0.9922 - val_loss: 0.2904 - val_acc: 0.9434\n",
            "Epoch 3/100\n",
            "3958/3958 [==============================] - 1s 238us/step - loss: 0.0283 - acc: 0.9914 - val_loss: 0.2562 - val_acc: 0.9495\n",
            "Epoch 4/100\n",
            "3958/3958 [==============================] - 1s 242us/step - loss: 0.0461 - acc: 0.9879 - val_loss: 0.2891 - val_acc: 0.9505\n",
            "Epoch 5/100\n",
            "3958/3958 [==============================] - 1s 250us/step - loss: 0.0460 - acc: 0.9826 - val_loss: 0.2645 - val_acc: 0.9525\n",
            "Epoch 6/100\n",
            "3958/3958 [==============================] - 1s 248us/step - loss: 0.0305 - acc: 0.9912 - val_loss: 0.2620 - val_acc: 0.9535\n",
            "Epoch 7/100\n",
            "3958/3958 [==============================] - 1s 250us/step - loss: 0.0199 - acc: 0.9939 - val_loss: 0.2755 - val_acc: 0.9444\n",
            "Epoch 8/100\n",
            "3958/3958 [==============================] - 1s 245us/step - loss: 0.0217 - acc: 0.9937 - val_loss: 0.2774 - val_acc: 0.9485\n",
            "Epoch 9/100\n",
            "3958/3958 [==============================] - 1s 251us/step - loss: 0.0198 - acc: 0.9934 - val_loss: 0.2866 - val_acc: 0.9414\n",
            "Epoch 10/100\n",
            "3958/3958 [==============================] - 1s 240us/step - loss: 0.0178 - acc: 0.9934 - val_loss: 0.2584 - val_acc: 0.9485\n",
            "Epoch 11/100\n",
            "3958/3958 [==============================] - 1s 239us/step - loss: 0.0193 - acc: 0.9949 - val_loss: 0.2930 - val_acc: 0.9525\n",
            "Epoch 12/100\n",
            "3958/3958 [==============================] - 1s 238us/step - loss: 0.0347 - acc: 0.9881 - val_loss: 0.2939 - val_acc: 0.9475\n",
            "Epoch 13/100\n",
            "3958/3958 [==============================] - 1s 260us/step - loss: 0.0213 - acc: 0.9929 - val_loss: 0.2384 - val_acc: 0.9535\n",
            "Epoch 14/100\n",
            "3958/3958 [==============================] - 1s 255us/step - loss: 0.0163 - acc: 0.9944 - val_loss: 0.2443 - val_acc: 0.9556\n",
            "Epoch 15/100\n",
            "3958/3958 [==============================] - 1s 259us/step - loss: 0.0269 - acc: 0.9901 - val_loss: 0.2691 - val_acc: 0.9566\n",
            "Epoch 16/100\n",
            "3958/3958 [==============================] - 1s 254us/step - loss: 0.0281 - acc: 0.9909 - val_loss: 0.2757 - val_acc: 0.9525\n",
            "Epoch 17/100\n",
            "3958/3958 [==============================] - 1s 248us/step - loss: 0.0123 - acc: 0.9960 - val_loss: 0.2551 - val_acc: 0.9576\n",
            "Epoch 18/100\n",
            "3958/3958 [==============================] - 1s 257us/step - loss: 0.0199 - acc: 0.9947 - val_loss: 0.2514 - val_acc: 0.9515\n",
            "Epoch 19/100\n",
            "3958/3958 [==============================] - 1s 265us/step - loss: 0.0263 - acc: 0.9907 - val_loss: 0.2571 - val_acc: 0.9556\n",
            "Epoch 20/100\n",
            "3958/3958 [==============================] - 1s 247us/step - loss: 0.0364 - acc: 0.9886 - val_loss: 0.2893 - val_acc: 0.9515\n",
            "Epoch 21/100\n",
            "3958/3958 [==============================] - 1s 260us/step - loss: 0.0235 - acc: 0.9912 - val_loss: 0.2291 - val_acc: 0.9535\n",
            "Epoch 22/100\n",
            "3958/3958 [==============================] - 1s 243us/step - loss: 0.0180 - acc: 0.9934 - val_loss: 0.2380 - val_acc: 0.9515\n",
            "Epoch 23/100\n",
            "3958/3958 [==============================] - 1s 256us/step - loss: 0.0276 - acc: 0.9904 - val_loss: 0.2459 - val_acc: 0.9566\n",
            "Epoch 24/100\n",
            "3958/3958 [==============================] - 1s 239us/step - loss: 0.0191 - acc: 0.9947 - val_loss: 0.3041 - val_acc: 0.9515\n",
            "Epoch 25/100\n",
            "3958/3958 [==============================] - 1s 239us/step - loss: 0.0328 - acc: 0.9881 - val_loss: 0.2735 - val_acc: 0.9525\n",
            "Epoch 26/100\n",
            "3958/3958 [==============================] - 1s 237us/step - loss: 0.0201 - acc: 0.9934 - val_loss: 0.2965 - val_acc: 0.9505\n",
            "Epoch 27/100\n",
            "3958/3958 [==============================] - 1s 252us/step - loss: 0.0283 - acc: 0.9927 - val_loss: 0.2781 - val_acc: 0.9626\n",
            "Epoch 28/100\n",
            "3958/3958 [==============================] - 1s 242us/step - loss: 0.0221 - acc: 0.9927 - val_loss: 0.2721 - val_acc: 0.9455\n",
            "Epoch 29/100\n",
            "3958/3958 [==============================] - 1s 254us/step - loss: 0.0207 - acc: 0.9932 - val_loss: 0.2749 - val_acc: 0.9596\n",
            "Epoch 30/100\n",
            "3958/3958 [==============================] - 1s 255us/step - loss: 0.0270 - acc: 0.9904 - val_loss: 0.2603 - val_acc: 0.9465\n",
            "Epoch 31/100\n",
            "3958/3958 [==============================] - 1s 257us/step - loss: 0.0260 - acc: 0.9917 - val_loss: 0.2713 - val_acc: 0.9515\n",
            "Epoch 32/100\n",
            "3958/3958 [==============================] - 1s 257us/step - loss: 0.0136 - acc: 0.9962 - val_loss: 0.2367 - val_acc: 0.9596\n",
            "Epoch 33/100\n",
            "3958/3958 [==============================] - 1s 250us/step - loss: 0.0154 - acc: 0.9947 - val_loss: 0.2367 - val_acc: 0.9535\n",
            "Epoch 34/100\n",
            "3958/3958 [==============================] - 1s 270us/step - loss: 0.0267 - acc: 0.9922 - val_loss: 0.2595 - val_acc: 0.9566\n",
            "Epoch 35/100\n",
            "3958/3958 [==============================] - 1s 262us/step - loss: 0.0185 - acc: 0.9937 - val_loss: 0.2706 - val_acc: 0.9535\n",
            "Epoch 36/100\n",
            "3958/3958 [==============================] - 1s 257us/step - loss: 0.0164 - acc: 0.9937 - val_loss: 0.3082 - val_acc: 0.9485\n",
            "Epoch 37/100\n",
            "3958/3958 [==============================] - 1s 244us/step - loss: 0.0179 - acc: 0.9937 - val_loss: 0.2394 - val_acc: 0.9535\n",
            "Epoch 38/100\n",
            "3958/3958 [==============================] - 1s 233us/step - loss: 0.0236 - acc: 0.9922 - val_loss: 0.3281 - val_acc: 0.9495\n",
            "Epoch 39/100\n",
            "3958/3958 [==============================] - 1s 246us/step - loss: 0.0354 - acc: 0.9866 - val_loss: 0.2585 - val_acc: 0.9404\n",
            "Epoch 40/100\n",
            "3958/3958 [==============================] - 1s 238us/step - loss: 0.0275 - acc: 0.9917 - val_loss: 0.3262 - val_acc: 0.9485\n",
            "Epoch 41/100\n",
            "3958/3958 [==============================] - 1s 245us/step - loss: 0.0206 - acc: 0.9914 - val_loss: 0.3504 - val_acc: 0.9475\n",
            "Epoch 42/100\n",
            "3958/3958 [==============================] - 1s 250us/step - loss: 0.0213 - acc: 0.9929 - val_loss: 0.3156 - val_acc: 0.9515\n",
            "Epoch 43/100\n",
            "3958/3958 [==============================] - 1s 244us/step - loss: 0.0314 - acc: 0.9927 - val_loss: 0.3819 - val_acc: 0.9455\n",
            "Epoch 44/100\n",
            "3958/3958 [==============================] - 1s 257us/step - loss: 0.0190 - acc: 0.9937 - val_loss: 0.3200 - val_acc: 0.9465\n",
            "Epoch 45/100\n",
            "3958/3958 [==============================] - 1s 268us/step - loss: 0.0212 - acc: 0.9934 - val_loss: 0.3195 - val_acc: 0.9475\n",
            "Epoch 46/100\n",
            "3958/3958 [==============================] - 1s 248us/step - loss: 0.0379 - acc: 0.9886 - val_loss: 0.3439 - val_acc: 0.9394\n",
            "Epoch 47/100\n",
            "3958/3958 [==============================] - 1s 245us/step - loss: 0.0295 - acc: 0.9919 - val_loss: 0.3000 - val_acc: 0.9545\n",
            "Epoch 48/100\n",
            "3958/3958 [==============================] - 1s 244us/step - loss: 0.0068 - acc: 0.9977 - val_loss: 0.2969 - val_acc: 0.9556\n",
            "Epoch 49/100\n",
            "3958/3958 [==============================] - 1s 235us/step - loss: 0.0153 - acc: 0.9965 - val_loss: 0.3091 - val_acc: 0.9535\n",
            "Epoch 50/100\n",
            "3958/3958 [==============================] - 1s 235us/step - loss: 0.0069 - acc: 0.9987 - val_loss: 0.2862 - val_acc: 0.9535\n",
            "Epoch 51/100\n",
            "3958/3958 [==============================] - 1s 233us/step - loss: 0.0215 - acc: 0.9937 - val_loss: 0.3341 - val_acc: 0.9394\n",
            "Epoch 52/100\n",
            "3958/3958 [==============================] - 1s 241us/step - loss: 0.0448 - acc: 0.9856 - val_loss: 0.3405 - val_acc: 0.9525\n",
            "Epoch 53/100\n",
            "3958/3958 [==============================] - 1s 238us/step - loss: 0.0332 - acc: 0.9922 - val_loss: 0.3293 - val_acc: 0.9525\n",
            "Epoch 54/100\n",
            "3958/3958 [==============================] - 1s 254us/step - loss: 0.0212 - acc: 0.9932 - val_loss: 0.2852 - val_acc: 0.9596\n",
            "Epoch 55/100\n",
            "3958/3958 [==============================] - 1s 264us/step - loss: 0.0106 - acc: 0.9965 - val_loss: 0.2991 - val_acc: 0.9576\n",
            "Epoch 56/100\n",
            "3958/3958 [==============================] - 1s 260us/step - loss: 0.0075 - acc: 0.9972 - val_loss: 0.3474 - val_acc: 0.9545\n",
            "Epoch 57/100\n",
            "3958/3958 [==============================] - 1s 270us/step - loss: 0.0065 - acc: 0.9982 - val_loss: 0.2945 - val_acc: 0.9535\n",
            "Epoch 58/100\n",
            "3958/3958 [==============================] - 1s 268us/step - loss: 0.0088 - acc: 0.9970 - val_loss: 0.2917 - val_acc: 0.9444\n",
            "Epoch 59/100\n",
            "3958/3958 [==============================] - 1s 251us/step - loss: 0.0417 - acc: 0.9866 - val_loss: 0.3818 - val_acc: 0.9303\n",
            "Epoch 60/100\n",
            "3958/3958 [==============================] - 1s 261us/step - loss: 0.0373 - acc: 0.9907 - val_loss: 0.3210 - val_acc: 0.9505\n",
            "Epoch 61/100\n",
            "3958/3958 [==============================] - 1s 254us/step - loss: 0.0182 - acc: 0.9955 - val_loss: 0.2891 - val_acc: 0.9515\n",
            "Epoch 62/100\n",
            "3958/3958 [==============================] - 1s 246us/step - loss: 0.0391 - acc: 0.9889 - val_loss: 0.3109 - val_acc: 0.9566\n",
            "Epoch 63/100\n",
            "3958/3958 [==============================] - 1s 254us/step - loss: 0.0285 - acc: 0.9886 - val_loss: 0.3235 - val_acc: 0.9556\n",
            "Epoch 64/100\n",
            "3958/3958 [==============================] - 1s 251us/step - loss: 0.0189 - acc: 0.9955 - val_loss: 0.3244 - val_acc: 0.9455\n",
            "Epoch 65/100\n",
            "3958/3958 [==============================] - 1s 265us/step - loss: 0.0139 - acc: 0.9949 - val_loss: 0.2482 - val_acc: 0.9556\n",
            "Epoch 66/100\n",
            "3958/3958 [==============================] - 1s 263us/step - loss: 0.0195 - acc: 0.9932 - val_loss: 0.3082 - val_acc: 0.9495\n",
            "Epoch 67/100\n",
            "3958/3958 [==============================] - 1s 261us/step - loss: 0.0173 - acc: 0.9942 - val_loss: 0.2956 - val_acc: 0.9606\n",
            "Epoch 68/100\n",
            "3958/3958 [==============================] - 1s 251us/step - loss: 0.0126 - acc: 0.9972 - val_loss: 0.3125 - val_acc: 0.9576\n",
            "Epoch 69/100\n",
            "3958/3958 [==============================] - 1s 249us/step - loss: 0.0184 - acc: 0.9939 - val_loss: 0.2952 - val_acc: 0.9515\n",
            "Epoch 70/100\n",
            "3958/3958 [==============================] - 1s 252us/step - loss: 0.0254 - acc: 0.9919 - val_loss: 0.3201 - val_acc: 0.9485\n",
            "Epoch 71/100\n",
            "3958/3958 [==============================] - 1s 244us/step - loss: 0.0166 - acc: 0.9939 - val_loss: 0.2607 - val_acc: 0.9475\n",
            "Epoch 72/100\n",
            "3958/3958 [==============================] - 1s 252us/step - loss: 0.0292 - acc: 0.9896 - val_loss: 0.3131 - val_acc: 0.9515\n",
            "Epoch 73/100\n",
            "3958/3958 [==============================] - 1s 242us/step - loss: 0.0195 - acc: 0.9944 - val_loss: 0.3003 - val_acc: 0.9515\n",
            "Epoch 74/100\n",
            "3958/3958 [==============================] - 1s 245us/step - loss: 0.0224 - acc: 0.9942 - val_loss: 0.3321 - val_acc: 0.9596\n",
            "Epoch 75/100\n",
            "3958/3958 [==============================] - 1s 254us/step - loss: 0.0205 - acc: 0.9932 - val_loss: 0.3396 - val_acc: 0.9566\n",
            "Epoch 76/100\n",
            "3958/3958 [==============================] - 1s 250us/step - loss: 0.0246 - acc: 0.9922 - val_loss: 0.2691 - val_acc: 0.9646\n",
            "Epoch 77/100\n",
            "3958/3958 [==============================] - 1s 244us/step - loss: 0.0471 - acc: 0.9848 - val_loss: 0.3044 - val_acc: 0.9495\n",
            "Epoch 78/100\n",
            "3958/3958 [==============================] - 1s 247us/step - loss: 0.0185 - acc: 0.9944 - val_loss: 0.3107 - val_acc: 0.9545\n",
            "Epoch 79/100\n",
            "3958/3958 [==============================] - 1s 252us/step - loss: 0.0138 - acc: 0.9960 - val_loss: 0.2505 - val_acc: 0.9566\n",
            "Epoch 80/100\n",
            "3958/3958 [==============================] - 1s 249us/step - loss: 0.0071 - acc: 0.9985 - val_loss: 0.2647 - val_acc: 0.9576\n",
            "Epoch 81/100\n",
            "3958/3958 [==============================] - 1s 241us/step - loss: 0.0136 - acc: 0.9955 - val_loss: 0.2686 - val_acc: 0.9515\n",
            "Epoch 82/100\n",
            "3958/3958 [==============================] - 1s 244us/step - loss: 0.0289 - acc: 0.9922 - val_loss: 0.2935 - val_acc: 0.9545\n",
            "Epoch 83/100\n",
            "3958/3958 [==============================] - 1s 243us/step - loss: 0.0219 - acc: 0.9932 - val_loss: 0.3264 - val_acc: 0.9556\n",
            "Epoch 84/100\n",
            "3958/3958 [==============================] - 1s 242us/step - loss: 0.0199 - acc: 0.9937 - val_loss: 0.2963 - val_acc: 0.9556\n",
            "Epoch 85/100\n",
            "3958/3958 [==============================] - 1s 244us/step - loss: 0.0217 - acc: 0.9922 - val_loss: 0.3358 - val_acc: 0.9545\n",
            "Epoch 86/100\n",
            "3958/3958 [==============================] - 1s 241us/step - loss: 0.0069 - acc: 0.9982 - val_loss: 0.3088 - val_acc: 0.9525\n",
            "Epoch 87/100\n",
            "3958/3958 [==============================] - 1s 248us/step - loss: 0.0264 - acc: 0.9914 - val_loss: 0.3366 - val_acc: 0.9444\n",
            "Epoch 88/100\n",
            "3958/3958 [==============================] - 1s 244us/step - loss: 0.0306 - acc: 0.9891 - val_loss: 0.3340 - val_acc: 0.9566\n",
            "Epoch 89/100\n",
            "3958/3958 [==============================] - 1s 244us/step - loss: 0.0334 - acc: 0.9894 - val_loss: 0.3051 - val_acc: 0.9525\n",
            "Epoch 90/100\n",
            "3958/3958 [==============================] - 1s 254us/step - loss: 0.0226 - acc: 0.9919 - val_loss: 0.3100 - val_acc: 0.9525\n",
            "Epoch 91/100\n",
            "3958/3958 [==============================] - 1s 248us/step - loss: 0.0127 - acc: 0.9947 - val_loss: 0.3345 - val_acc: 0.9515\n",
            "Epoch 92/100\n",
            "3958/3958 [==============================] - 1s 253us/step - loss: 0.0129 - acc: 0.9967 - val_loss: 0.3153 - val_acc: 0.9495\n",
            "Epoch 93/100\n",
            "3958/3958 [==============================] - 1s 252us/step - loss: 0.0123 - acc: 0.9949 - val_loss: 0.3262 - val_acc: 0.9495\n",
            "Epoch 94/100\n",
            "3958/3958 [==============================] - 1s 256us/step - loss: 0.0254 - acc: 0.9919 - val_loss: 0.3048 - val_acc: 0.9465\n",
            "Epoch 95/100\n",
            "3958/3958 [==============================] - 1s 240us/step - loss: 0.0190 - acc: 0.9927 - val_loss: 0.2885 - val_acc: 0.9515\n",
            "Epoch 96/100\n",
            "3958/3958 [==============================] - 1s 239us/step - loss: 0.0179 - acc: 0.9929 - val_loss: 0.3727 - val_acc: 0.9414\n",
            "Epoch 97/100\n",
            "3958/3958 [==============================] - 1s 251us/step - loss: 0.0165 - acc: 0.9949 - val_loss: 0.3371 - val_acc: 0.9465\n",
            "Epoch 98/100\n",
            "3958/3958 [==============================] - 1s 261us/step - loss: 0.0140 - acc: 0.9955 - val_loss: 0.3251 - val_acc: 0.9485\n",
            "Epoch 99/100\n",
            "3958/3958 [==============================] - 1s 252us/step - loss: 0.0163 - acc: 0.9944 - val_loss: 0.3211 - val_acc: 0.9535\n",
            "Epoch 100/100\n",
            "3958/3958 [==============================] - 1s 248us/step - loss: 0.0141 - acc: 0.9962 - val_loss: 0.3820 - val_acc: 0.9394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPR3lVHImxsM",
        "colab_type": "code",
        "outputId": "806af91c-5158-4276-907b-32d265101ed4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model_name = 'OnlyNeutral_Emotion_Voice_Detection_Model.h5'\n",
        "save_dir = 'gdrive/My Drive/ravdess_model'\n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved trained model at gdrive/My Drive/ravdess_model/OnlyNeutral_Emotion_Voice_Detection_Model.h5 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0qXnF5jKh_o",
        "colab_type": "code",
        "outputId": "1634aec2-3a1f-4be8-99ac-e1db74528785",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(cnnhistory.history['loss'])\n",
        "plt.plot(cnnhistory.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OJQxXSC5UY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model = Sequential()\n",
        "\n",
        "# model.add(Conv1D(128, 5,padding='same',\n",
        "#                  input_shape=(40,1)))\n",
        "# model.add(Activation('relu'))\n",
        "# model.add(Dropout(0.1))\n",
        "# model.add(MaxPooling1D(pool_size=(8)))\n",
        "# model.add(Conv1D(128, 5,padding='same',))\n",
        "# model.add(Activation('relu'))\n",
        "# model.add(Dropout(0.1))\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(8))\n",
        "# model.add(Activation('softmax'))\n",
        "# opt = keras.optimizers.rmsprop(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXEncYELgZGm",
        "colab_type": "code",
        "outputId": "1f9e0168-77db-41a5-b8de-50eceb181be2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "predictions = model.predict_classes(x_validcnn)\n",
        "print(predictions)\n",
        "solutions = model.predict(x_validcnn)\n",
        "print(solutions[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2 4 3 0 ... 2 3 1 5]\n",
            "[1.551546e-13 1.539131e-02 9.846087e-01 1.121310e-16 4.077830e-19 9.010878e-15 8.935554e-19 7.600190e-11]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqNi2bo5gg_k",
        "colab_type": "code",
        "outputId": "f9e7176f-35db-42b8-eff7-41c6720f1dbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "report = classification_report(valid_y, predictions)\n",
        "print(report)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.90      0.93        84\n",
            "           1       0.92      0.95      0.93       165\n",
            "           2       0.89      0.94      0.91       132\n",
            "           3       0.95      0.95      0.95       156\n",
            "           4       0.97      0.94      0.96       141\n",
            "           5       0.93      0.93      0.93       150\n",
            "           6       0.95      0.94      0.95        87\n",
            "           7       1.00      0.95      0.97        75\n",
            "\n",
            "    accuracy                           0.94       990\n",
            "   macro avg       0.94      0.94      0.94       990\n",
            "weighted avg       0.94      0.94      0.94       990\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf1l02Ccgy-1",
        "colab_type": "code",
        "outputId": "18a96009-8d0c-4a5c-9bd4-14fca9ac0006",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "matrix = confusion_matrix(valid_y, predictions)\n",
        "print (matrix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 76   2   0   2   0   4   0   0]\n",
            " [  0 157   7   1   0   0   0   0]\n",
            " [  4   2 124   0   0   2   0   0]\n",
            " [  0   8   0 148   0   0   0   0]\n",
            " [  0   0   0   0 133   4   4   0]\n",
            " [  0   0   5   4   2 139   0   0]\n",
            " [  0   2   2   1   0   0  82   0]\n",
            " [  0   0   2   0   2   0   0  71]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMLWOaAqhqnk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = 'Emotion_Voice_Detection_Model.h5'\n",
        "save_dir = 'gdrive/My Drive/ravdess_model'\n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWGjo_-Xh4XT",
        "colab_type": "code",
        "outputId": "420b7e74-5daa-4302-bff6-388db43250c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "source": [
        "loaded_model = keras.models.load_model('gdrive/My Drive/ravdess_model/OnlyNeutral_Emotion_Voice_Detection_Model.h5')\n",
        "loaded_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_1 (Conv1D)            (None, 40, 128)           768       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 5, 128)            82048     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 128)            0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 640)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                41024     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 8)                 520       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 8)                 0         \n",
            "=================================================================\n",
            "Total params: 124,360\n",
            "Trainable params: 124,360\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDDuRKZzh-FZ",
        "colab_type": "code",
        "outputId": "07b92947-4017-465d-a65e-cdfae4126b9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "loss, acc = loaded_model.evaluate(x_validcnn, valid_y)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "990/990 [==============================] - 0s 158us/step\n",
            "Restored model, accuracy: 92.63%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhrikIJP9kyq",
        "colab_type": "code",
        "outputId": "4be743f9-ea14-490a-caca-747b580fcdc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import time \n",
        "import librosa\n",
        "import os\n",
        "start_time = time.time()\n",
        "lst = []\n",
        "folder = 'gdrive/My Drive/ravdess_model/data'\n",
        "for file in os.listdir(folder):\n",
        "  try:\n",
        "    X, sample_rate = librosa.load(os.path.join(folder,file), res_type='kaiser_fast')\n",
        "    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0) \n",
        "    file = file[0] \n",
        "    arr = mfccs, file\n",
        "    lst.append(arr)\n",
        "  except ValueError:\n",
        "    continue\n",
        "\n",
        "print(\"--- Data loaded. Loading time: %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Data loaded. Loading time: 17.29483461380005 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZoTDbOQ-B6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_X, test_y = zip(*lst)\n",
        "test_X = np.asarray(test_X)\n",
        "# test_y = np.asarray(test_y)\n",
        "\n",
        "\n",
        "\n",
        "test_X = np.expand_dims(test_X, axis=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2e5DDJz-3CW",
        "colab_type": "code",
        "outputId": "acda0962-2bde-4bb0-9b39-c62bfadf8b57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "classes = ['neutral','happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']\n",
        "\n",
        "\n",
        "# print(test_y)\n",
        "solutions = loaded_model.predict(test_X)\n",
        "\n",
        "# print(solutions[0])\n",
        "# for i,class_ in enumerate(classes):\n",
        "#   print(i,class_)\n",
        "  \n",
        "for i in range(len(solutions)):\n",
        "  indices = solutions[i].argsort()[-2:][::-1]\n",
        "  for j in indices:\n",
        "    if j==1:\n",
        "      j=0\n",
        "    print(classes[j])\n",
        "  print('----')\n",
        "  \n",
        "# # Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
        "# # 0 Neutral 1 calm 2 happy 3 sad 4 angry 5 fearful 6 disgust 7 is surprised"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-ab2ba7bcc636>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# print(test_y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msolutions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# print(solutions[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'loaded_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7ZMHJYbGxjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVtfiKXDjjwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Submission Code \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmgP_PqMAlF2",
        "colab_type": "text"
      },
      "source": [
        "2 4 2 0 2 0 0 0"
      ]
    }
  ]
}